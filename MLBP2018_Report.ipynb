{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning Basic Principles 2018 - Data Analysis Project Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *A Comparison of Support Vector Machines and XGBoosting  in Music Genre Classification* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements:\n",
    "``pip3 install seaborn``  \n",
    "``pip3 install xgboost``  \n",
    "``pip3 install prince``  \n",
    "``pip3 install matplotlib``  \n",
    "``pip3 install -U scikit-learn``  \n",
    "``pip3 install pandas``  \n",
    "``pip3 install numpy``  \n",
    "``pip3 install ipywidgets``  \n",
    "``pip3 install ipython``\n",
    "\n",
    "**Running time warning**  \n",
    "*The model fitting may take a long time depending on the model. However we have run the notebook beforehand and you should be able to see the prints of the scores etc. so you don't have to run the fits again. Another note is that the Data Analysis section contains ipywidgets that will make the cells interactive. These cells should be run again to see the results of the data analysis and there should not be any running time issues with these.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This report contains the creation and testing of two complete machine learning  solutions for classifying music according to genre. The aim is to find out more about the performance of a hyped machine learning method called XGBoost versus a more traditional one, here Support vector Machines, in a high dimensional classification problem where the data is imbalanced such as the data in this problem.The first part of the report contains the data analysis of pre-processed audio data. The next part consists of processing the data and fitting the actual models as well as testing their performance. The solutions are also run on kaggle to see how they perform on test data, and scored based on their accuracy and log loss. The solutions in the accuracy scores are Kernel Support Vector Machines with Grid Search For Multiclass Classification and XGBoost Gradient Boosting with Grid Search and Hyperparameter Fine Tuning, while the solutions for the log loss competition are Linear Support Vector Machines for Multiclass Probability Classification and XGBoost for Multiclass Probability Classification. After analysing and ranking the scores it is apparent that Support Vector Machines perform better in both competitions and also outperform XGBoosting in training running time. XGBoosting only outperforms Support Vector Machines in prediction running time and in the accuracy and log loss competitions when the data is not normalized.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The goal is to learn how to implement a complete machine learning solution using the tools that have been taught during the course as well as implementing new tools such as the library scikit learn. In this project we will be designing a complete machine learning solution for classifying music according to genre.*  \n",
    "  \n",
    "*So, what is sound? Sound is a sequence of pressure waves that propagate through a medium such as air or water. When this pressure wave hits your ear, a series of biological reactions occur, that result in your brain interpreting it as a sound. In this project the pressure waves of different songs have been converted to electrical signals and further into a discrete digital signal. The data has then been further preprocessed into a form that can be readily used for developing a machine learning method [[1](#[1])].*   \n",
    "\n",
    "*In this project a predictor h(x) is created for each genre Y, which takes the features x of the pre-processed songs, and and computes the probability h(x) that the song belongs to a specific genre, e.g. “Rap”. The genres used in this project are “Pop Rock”, “Electronic”, “Rap”, “Jazz”, “Latin”, “RnB”, “International”, “Country”, “Reggae” and “Blues”. A pre-processed data set of songs with carefully chosen features are used to compute a predictor.*  \n",
    "  \n",
    "*Using this method of grouping songs can be useful when putting music into categories. Companies such as Youtube and Spotify can use this kind of categorisation for recommendation or discovery services. However, classifying music according to genre is not easy because some genres can be fairly similar, e.g. rock and blues. To achieve an as accurate as possible model three main components of music are compared: timbre, pitch and rhythm.*  \n",
    "  \n",
    "*The first part of the project consists of analysing and processing the data and dividing it up into features timbre, pitch and rhythm. Then we will be moving on to fitting a model. We were particularly intrested in the performance of a hyped machine learning method called XGBoost versus a more traditional one in a high dimensional classification problem where the data is imbalanced such as the data in this problem. In the end we chose to compare XGBoost with hyperparameter fine tuning against Support Vector Machines. These methods are used for both the accuracy competition and the logloss competition. Grid search is used for all the models. Finally we visualize our results and discuss the results and the performance of our solution. We wish to find out which method gives us the best results when applied to the test data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data analysis  \n",
    "  \n",
    "*We will begin with visualising the pre-processed visual data. Based on the results of the visualisation we have the options of choosing how to scale it, and possibly get rid of faulty data points or features. Our assumption is that the training data and the test data are similar. After separating the data into three features (rhythm, chroma and mfcc), we will further multi-index the data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Quick view of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x0E4DBDB0>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAD/tJREFUeJzt3X+MZWV9x/H3p6y2ClogyIQC6dJka0WJSCdAJWmG0uACTZcmJYFSWShmmxZabTZpV/+h0ZjwR7WtiSXd6pa1tWyIP8JGNtLN1onxDyygKCAqG1xhYctqQXSxqV377R9ztntnmWVm7p25Z3af9yuZ3Hu/85x7vveBmc+e55x7J1WFJKk9P9N3A5KkfhgAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgLRISU5N8tkkLyX5bpLf67snaRir+m5AOgZ9FPgJMAGcD9yb5GtV9Vi/bUmLE98JLC1ckhOBF4C3VNW3u9o/Ac9U1aZem5MWySUgaXF+GfjpoV/+na8Bb+6pH2loBoC0OCcBLx5RexF4XQ+9SCMxAKTFOQC8/oja64Ef9dCLNBIDQFqcbwOrkqwZqL0V8ASwjjmeBJYWKck2oIB3MXMV0A7g7V4FpGONRwDS4v0x8BpgP3AX8Ef+8texyCMASWqURwCS1CgDQJIaZQBIUqMMAElq1Ir+MLjTTjutVq9e3XcbI3nppZc48cQT+25jxXA+ZnM+DnMuZhtlPh566KHvV9Ub5hu3ogNg9erVPPjgg323MZLp6Wmmpqb6bmPFcD5mcz4Ocy5mG2U+knx3IeNcApKkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEat6HcCj2r1pnuH3nbP7VctYSeStPJ4BCBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRs0bAEnOTvKFJI8neSzJu7v6qUl2Jnmiuz2lqyfJR5LsTvL1JBcMPNf6bvwTSdYv38uSJM1nIUcAB4GNVfUm4GLgliTnApuAXVW1BtjVPQa4AljTfW0A7oCZwABuAy4CLgRuOxQakqTxmzcAqmpfVX2lu/8j4HHgTGAdsLUbthW4uru/DvhEzbgfODnJGcA7gJ1V9XxVvQDsBNYu6auRJC3Yos4BJFkNvA34MjBRVftgJiSA07thZwJPD2y2t6sdrS5J6sGC/yBMkpOATwPvqaofJjnq0Dlq9Qr1I/ezgZmlIyYmJpienl5oiy+z8byDQ287yn4HHThwYMme63jgfMzmfBzmXMw2jvlYUAAkeRUzv/w/WVWf6crPJTmjqvZ1Szz7u/pe4OyBzc8Cnu3qU0fUp4/cV1VtBjYDTE5O1tTU1JFDFuzGUf4i2PXD73fQ9PQ0o7yG443zMZvzcZhzMds45mMhVwEF+DjweFV9eOBb24FDV/KsB+4ZqN/QXQ10MfBit0R0H3B5klO6k7+XdzVJUg8WcgRwCfBO4JEkD3e19wG3A3cnuRl4Crim+94O4EpgN/Bj4CaAqno+yQeAB7px76+q55fkVUiSFm3eAKiqLzH3+j3AZXOML+CWozzXFmDLYhqUJC0P3wksSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSo+YNgCRbkuxP8uhA7S+TPJPk4e7ryoHvvTfJ7iTfSvKOgfrarrY7yaalfymSpMVYyBHAncDaOep/XVXnd187AJKcC1wLvLnb5u+SnJDkBOCjwBXAucB13VhJUk9WzTegqr6YZPUCn28dsK2q/hv4TpLdwIXd93ZX1ZMASbZ1Y7+x6I4lSUti3gB4BbcmuQF4ENhYVS8AZwL3D4zZ29UAnj6iftFcT5pkA7ABYGJigunp6aEb3HjewaG3HWW/gw4cOLBkz3U8cD5mcz4Ocy5mG8d8DBsAdwAfAKq7/RDwB0DmGFvMvdRUcz1xVW0GNgNMTk7W1NTUkC3CjZvuHXrbPdcPv99B09PTjPIajjfOx2zOx2HOxWzjmI+hAqCqnjt0P8k/AJ/rHu4Fzh4YehbwbHf/aHVJUg+Gugw0yRkDD38HOHSF0Hbg2iQ/m+QcYA3w78ADwJok5yR5NTMnircP37YkaVTzHgEkuQuYAk5Lshe4DZhKcj4zyzh7gD8EqKrHktzNzMndg8AtVfXT7nluBe4DTgC2VNVjS/5qJEkLtpCrgK6bo/zxVxj/QeCDc9R3ADsW1Z0kadn4TmBJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaNW8AJNmSZH+SRwdqpybZmeSJ7vaUrp4kH0myO8nXk1wwsM36bvwTSdYvz8uRJC3UQo4A7gTWHlHbBOyqqjXAru4xwBXAmu5rA3AHzAQGcBtwEXAhcNuh0JAk9WPeAKiqLwLPH1FeB2zt7m8Frh6of6Jm3A+cnOQM4B3Azqp6vqpeAHby8lCRJI3RqiG3m6iqfQBVtS/J6V39TODpgXF7u9rR6i+TZAMzRw9MTEwwPT09ZIuw8byDQ287yn4HHThwYMme63jgfMzmfBzmXMw2jvkYNgCOJnPU6hXqLy9WbQY2A0xOTtbU1NTQzdy46d6ht91z/fD7HTQ9Pc0or+F443zM5nwc5lzMNo75GPYqoOe6pR262/1dfS9w9sC4s4BnX6EuSerJsAGwHTh0Jc964J6B+g3d1UAXAy92S0X3AZcnOaU7+Xt5V5Mk9WTeJaAkdwFTwGlJ9jJzNc/twN1JbgaeAq7phu8ArgR2Az8GbgKoqueTfAB4oBv3/qo68sSyJGmM5g2AqrruKN+6bI6xBdxylOfZAmxZVHeSpGXjO4ElqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSo1b13cDxaPWme////sbzDnLjwOP57Ln9quVoSZJexiMASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaNVIAJNmT5JEkDyd5sKudmmRnkie621O6epJ8JMnuJF9PcsFSvABJ0nCW4gjg0qo6v6omu8ebgF1VtQbY1T0GuAJY031tAO5Ygn1Lkoa0HEtA64Ct3f2twNUD9U/UjPuBk5OcsQz7lyQtQKpq+I2T7wAvAAX8fVVtTvKDqjp5YMwLVXVKks8Bt1fVl7r6LuAvqurBI55zAzNHCExMTPzqtm3bhu7vkWdeHHrb8878+SXZ78Rr4Ln/Gs9+jwUHDhzgpJNO6ruNFcP5OMy5mG2U+bj00ksfGliVOapRPwvokqp6NsnpwM4k33yFsZmj9rL0qarNwGaAycnJmpqaGrq5xXwGz5H2XL80+9143kE+9MjCp3mU/R4LpqenGeW/6fHG+TjMuZhtHPMx0hJQVT3b3e4HPgtcCDx3aGmnu93fDd8LnD2w+VnAs6PsX5I0vKEDIMmJSV536D5wOfAosB1Y3w1bD9zT3d8O3NBdDXQx8GJV7Ru6c0nSSEZZApoAPpvk0PP8S1V9PskDwN1JbgaeAq7pxu8ArgR2Az8Gbhph35KkEQ0dAFX1JPDWOer/CVw2R72AW4bdnyRpaflOYElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVGj/j0ArTCrR/kbCLdftYSdSFrpPAKQpEYZAJLUKANAkhplAEhSowwASWqUVwFJPTh0tdbG8w5y4yKv3PJqLS0VA0BLYqGXn871C89faFI/XAKSpEYZAJLUKJeAdEwb5Z3P4PKT2uYRgCQ1yiMASWMx39HaK10R5ZHa8vAIQJIaZQBIUqMMAElqlAEgSY3yJLDUGP9okA7xCECSGmUASFKjDABJapQBIEmN8iSwpOOanxd1dAaAJC2TUcLnzrUnLmEnc3MJSJIaZQBIUqMMAElqlAEgSY0aewAkWZvkW0l2J9k07v1LkmaMNQCSnAB8FLgCOBe4Lsm54+xBkjRj3EcAFwK7q+rJqvoJsA1YN+YeJElAqmp8O0t+F1hbVe/qHr8TuKiqbh0YswHY0D18I/CtsTW4PE4Dvt93EyuI8zGb83GYczHbKPPxi1X1hvkGjfuNYJmjNiuBqmozsHk87Sy/JA9W1WTffawUzsdszsdhzsVs45iPcS8B7QXOHnh8FvDsmHuQJDH+AHgAWJPknCSvBq4Fto+5B0kSY14CqqqDSW4F7gNOALZU1WPj7KEHx81y1hJxPmZzPg5zLmZb9vkY60lgSdLK4TuBJalRBoAkNcoAWCZJzk7yhSSPJ3ksybv77qlvSU5I8tUkn+u7l74lOTnJp5J8s/t/5Nf67qlPSf6s+zl5NMldSX6u757GKcmWJPuTPDpQOzXJziRPdLenLPV+DYDlcxDYWFVvAi4GbvFjL3g38HjfTawQfwt8vqp+BXgrDc9LkjOBPwUmq+otzFwgcm2/XY3dncDaI2qbgF1VtQbY1T1eUgbAMqmqfVX1le7+j5j5AT+z3676k+Qs4CrgY3330rckrwd+Hfg4QFX9pKp+0G9XvVsFvCbJKuC1NPb+oKr6IvD8EeV1wNbu/lbg6qXerwEwBklWA28DvtxvJ736G+DPgf/tu5EV4JeA7wH/2C2JfSzJ8v/9vxWqqp4B/gp4CtgHvFhV/9pvVyvCRFXtg5l/UAKnL/UODIBlluQk4NPAe6rqh33304ckvwXsr6qH+u5lhVgFXADcUVVvA15iGQ7vjxXd2vY64BzgF4ATk/x+v121wQBYRklexcwv/09W1Wf67qdHlwC/nWQPM58A+xtJ/rnflnq1F9hbVYeOCD/FTCC06jeB71TV96rqf4DPAG/vuaeV4LkkZwB0t/uXegcGwDJJEmbWeB+vqg/33U+fquq9VXVWVa1m5uTev1VVs//Cq6r/AJ5O8saudBnwjR5b6ttTwMVJXtv93FxGwyfFB2wH1nf31wP3LPUOxv1poC25BHgn8EiSh7va+6pqR489aeX4E+CT3WdiPQnc1HM/vamqLyf5FPAVZq6e+yqNfSxEkruAKeC0JHuB24DbgbuT3MxMSF6z5Pv1oyAkqU0uAUlSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1Kj/A5y3UaiP1HZXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = pd.read_csv('test_data.csv', header=None)\n",
    "train = pd.read_csv('train_data.csv', header=None)\n",
    "train_labels = pd.read_csv('train_labels.csv', header=None)\n",
    "train_labels.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We see that most of the training labels are labelled as one. Our resulting label distribution should be similar to this histogram. Let's see what the data looks like by printing the first ten rows of the training set.*  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1040.70</td>\n",
       "      <td>2315.6</td>\n",
       "      <td>2839.1</td>\n",
       "      <td>2552.2</td>\n",
       "      <td>2290.4</td>\n",
       "      <td>1913.8</td>\n",
       "      <td>2152.6</td>\n",
       "      <td>1930.3</td>\n",
       "      <td>2079.3</td>\n",
       "      <td>1706.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.216490</td>\n",
       "      <td>0.365480</td>\n",
       "      <td>0.093584</td>\n",
       "      <td>0.166870</td>\n",
       "      <td>0.083426</td>\n",
       "      <td>0.118090</td>\n",
       "      <td>0.089792</td>\n",
       "      <td>0.074371</td>\n",
       "      <td>0.073162</td>\n",
       "      <td>0.059463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2309.40</td>\n",
       "      <td>4780.4</td>\n",
       "      <td>4055.7</td>\n",
       "      <td>3120.5</td>\n",
       "      <td>1979.9</td>\n",
       "      <td>2343.6</td>\n",
       "      <td>2634.2</td>\n",
       "      <td>3208.5</td>\n",
       "      <td>3078.0</td>\n",
       "      <td>3374.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100670</td>\n",
       "      <td>0.147390</td>\n",
       "      <td>0.102560</td>\n",
       "      <td>0.213040</td>\n",
       "      <td>0.082041</td>\n",
       "      <td>0.080967</td>\n",
       "      <td>0.076450</td>\n",
       "      <td>0.052523</td>\n",
       "      <td>0.052357</td>\n",
       "      <td>0.055297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2331.90</td>\n",
       "      <td>4607.0</td>\n",
       "      <td>4732.3</td>\n",
       "      <td>5007.0</td>\n",
       "      <td>3164.9</td>\n",
       "      <td>3171.9</td>\n",
       "      <td>2915.7</td>\n",
       "      <td>3282.3</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>1895.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126760</td>\n",
       "      <td>0.363210</td>\n",
       "      <td>0.114200</td>\n",
       "      <td>0.223780</td>\n",
       "      <td>0.100770</td>\n",
       "      <td>0.186910</td>\n",
       "      <td>0.067270</td>\n",
       "      <td>0.061138</td>\n",
       "      <td>0.085509</td>\n",
       "      <td>0.049422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3350.90</td>\n",
       "      <td>6274.4</td>\n",
       "      <td>5037.0</td>\n",
       "      <td>4609.7</td>\n",
       "      <td>3438.8</td>\n",
       "      <td>3925.8</td>\n",
       "      <td>3746.4</td>\n",
       "      <td>3539.4</td>\n",
       "      <td>3053.7</td>\n",
       "      <td>3075.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096479</td>\n",
       "      <td>0.289500</td>\n",
       "      <td>0.074124</td>\n",
       "      <td>0.201580</td>\n",
       "      <td>0.049032</td>\n",
       "      <td>0.130210</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.080885</td>\n",
       "      <td>0.148910</td>\n",
       "      <td>0.042027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017.60</td>\n",
       "      <td>3351.8</td>\n",
       "      <td>2924.9</td>\n",
       "      <td>2726.3</td>\n",
       "      <td>1979.9</td>\n",
       "      <td>1930.9</td>\n",
       "      <td>2083.4</td>\n",
       "      <td>1889.2</td>\n",
       "      <td>1695.4</td>\n",
       "      <td>1911.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138340</td>\n",
       "      <td>0.382660</td>\n",
       "      <td>0.079402</td>\n",
       "      <td>0.063495</td>\n",
       "      <td>0.053717</td>\n",
       "      <td>0.086750</td>\n",
       "      <td>0.062090</td>\n",
       "      <td>0.048999</td>\n",
       "      <td>0.033159</td>\n",
       "      <td>0.070813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1229.80</td>\n",
       "      <td>3005.8</td>\n",
       "      <td>2818.4</td>\n",
       "      <td>2640.1</td>\n",
       "      <td>2329.1</td>\n",
       "      <td>2568.4</td>\n",
       "      <td>2772.1</td>\n",
       "      <td>3119.3</td>\n",
       "      <td>2505.8</td>\n",
       "      <td>2085.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137290</td>\n",
       "      <td>0.065876</td>\n",
       "      <td>0.078278</td>\n",
       "      <td>0.058903</td>\n",
       "      <td>0.051245</td>\n",
       "      <td>0.049138</td>\n",
       "      <td>0.070669</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.053383</td>\n",
       "      <td>0.037763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3936.10</td>\n",
       "      <td>6276.3</td>\n",
       "      <td>4768.8</td>\n",
       "      <td>4157.2</td>\n",
       "      <td>3658.2</td>\n",
       "      <td>3830.3</td>\n",
       "      <td>4421.3</td>\n",
       "      <td>3712.8</td>\n",
       "      <td>3419.9</td>\n",
       "      <td>3437.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>0.304990</td>\n",
       "      <td>0.033861</td>\n",
       "      <td>0.033922</td>\n",
       "      <td>0.072498</td>\n",
       "      <td>0.153120</td>\n",
       "      <td>0.042135</td>\n",
       "      <td>0.104470</td>\n",
       "      <td>0.040248</td>\n",
       "      <td>0.028059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>556.39</td>\n",
       "      <td>1739.0</td>\n",
       "      <td>2072.0</td>\n",
       "      <td>2713.0</td>\n",
       "      <td>1828.7</td>\n",
       "      <td>2875.6</td>\n",
       "      <td>1739.8</td>\n",
       "      <td>1430.2</td>\n",
       "      <td>1531.4</td>\n",
       "      <td>785.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117100</td>\n",
       "      <td>0.096197</td>\n",
       "      <td>0.119490</td>\n",
       "      <td>0.164570</td>\n",
       "      <td>0.041037</td>\n",
       "      <td>0.118390</td>\n",
       "      <td>0.076247</td>\n",
       "      <td>0.079585</td>\n",
       "      <td>0.051912</td>\n",
       "      <td>0.094960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2764.70</td>\n",
       "      <td>4442.6</td>\n",
       "      <td>3930.6</td>\n",
       "      <td>3504.1</td>\n",
       "      <td>2421.8</td>\n",
       "      <td>3012.6</td>\n",
       "      <td>3154.7</td>\n",
       "      <td>2665.7</td>\n",
       "      <td>2846.5</td>\n",
       "      <td>2595.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175920</td>\n",
       "      <td>0.318960</td>\n",
       "      <td>0.039843</td>\n",
       "      <td>0.085944</td>\n",
       "      <td>0.062147</td>\n",
       "      <td>0.128530</td>\n",
       "      <td>0.070850</td>\n",
       "      <td>0.054834</td>\n",
       "      <td>0.127700</td>\n",
       "      <td>0.057441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3084.70</td>\n",
       "      <td>5645.9</td>\n",
       "      <td>5000.2</td>\n",
       "      <td>4001.9</td>\n",
       "      <td>2761.3</td>\n",
       "      <td>2946.5</td>\n",
       "      <td>2998.7</td>\n",
       "      <td>3141.1</td>\n",
       "      <td>3211.8</td>\n",
       "      <td>3374.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>0.114340</td>\n",
       "      <td>0.018940</td>\n",
       "      <td>0.230170</td>\n",
       "      <td>0.069801</td>\n",
       "      <td>0.041104</td>\n",
       "      <td>0.038691</td>\n",
       "      <td>0.078276</td>\n",
       "      <td>0.074719</td>\n",
       "      <td>0.059803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8    \\\n",
       "0  1040.70  2315.6  2839.1  2552.2  2290.4  1913.8  2152.6  1930.3  2079.3   \n",
       "1  2309.40  4780.4  4055.7  3120.5  1979.9  2343.6  2634.2  3208.5  3078.0   \n",
       "2  2331.90  4607.0  4732.3  5007.0  3164.9  3171.9  2915.7  3282.3  2400.0   \n",
       "3  3350.90  6274.4  5037.0  4609.7  3438.8  3925.8  3746.4  3539.4  3053.7   \n",
       "4  2017.60  3351.8  2924.9  2726.3  1979.9  1930.9  2083.4  1889.2  1695.4   \n",
       "5  1229.80  3005.8  2818.4  2640.1  2329.1  2568.4  2772.1  3119.3  2505.8   \n",
       "6  3936.10  6276.3  4768.8  4157.2  3658.2  3830.3  4421.3  3712.8  3419.9   \n",
       "7   556.39  1739.0  2072.0  2713.0  1828.7  2875.6  1739.8  1430.2  1531.4   \n",
       "8  2764.70  4442.6  3930.6  3504.1  2421.8  3012.6  3154.7  2665.7  2846.5   \n",
       "9  3084.70  5645.9  5000.2  4001.9  2761.3  2946.5  2998.7  3141.1  3211.8   \n",
       "\n",
       "       9      ...          254       255       256       257       258  \\\n",
       "0  1706.70    ...     0.216490  0.365480  0.093584  0.166870  0.083426   \n",
       "1  3374.70    ...     0.100670  0.147390  0.102560  0.213040  0.082041   \n",
       "2  1895.20    ...     0.126760  0.363210  0.114200  0.223780  0.100770   \n",
       "3  3075.40    ...     0.096479  0.289500  0.074124  0.201580  0.049032   \n",
       "4  1911.70    ...     0.138340  0.382660  0.079402  0.063495  0.053717   \n",
       "5  2085.00    ...     0.137290  0.065876  0.078278  0.058903  0.051245   \n",
       "6  3437.60    ...     0.139700  0.304990  0.033861  0.033922  0.072498   \n",
       "7   785.59    ...     0.117100  0.096197  0.119490  0.164570  0.041037   \n",
       "8  2595.20    ...     0.175920  0.318960  0.039843  0.085944  0.062147   \n",
       "9  3374.60    ...     0.123500  0.114340  0.018940  0.230170  0.069801   \n",
       "\n",
       "        259       260       261       262       263  \n",
       "0  0.118090  0.089792  0.074371  0.073162  0.059463  \n",
       "1  0.080967  0.076450  0.052523  0.052357  0.055297  \n",
       "2  0.186910  0.067270  0.061138  0.085509  0.049422  \n",
       "3  0.130210  0.045800  0.080885  0.148910  0.042027  \n",
       "4  0.086750  0.062090  0.048999  0.033159  0.070813  \n",
       "5  0.049138  0.070669  0.067383  0.053383  0.037763  \n",
       "6  0.153120  0.042135  0.104470  0.040248  0.028059  \n",
       "7  0.118390  0.076247  0.079585  0.051912  0.094960  \n",
       "8  0.128530  0.070850  0.054834  0.127700  0.057441  \n",
       "9  0.041104  0.038691  0.078276  0.074719  0.059803  \n",
       "\n",
       "[10 rows x 264 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The data at first glance doesn't make a lot of sense, so let's categorize further.*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Deeper Interactive Dive Into the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we will multi-index the data to make it easy to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rythm part\n",
    "rythm = train.iloc[:,0:168].copy()\n",
    "statistics_r = ['mean', 'median', 'variance', 'kurtosis', 'skewness', 'min', 'max']\n",
    "bands = [\"band_{}\".format(i) for i in range(24)]\n",
    "index_r = pd.MultiIndex.from_product([statistics_r, bands])\n",
    "rythm.columns = index_r\n",
    "\n",
    "# Chroma part\n",
    "chroma = train.iloc[:,168:216].copy()\n",
    "statistics_c = ['mean', 'std', 'min', 'max']\n",
    "pitches = [\"pitch_{}\".format(i) for i in range(12)]\n",
    "index_c = pd.MultiIndex.from_product([statistics_c, pitches])\n",
    "chroma.columns = index_c\n",
    "\n",
    "# MFCC part\n",
    "MFCC = train.iloc[:,216:].copy()\n",
    "statistics_m = ['mean', 'std', 'min', 'max']\n",
    "MFCCs = [\"MFCC_{}\".format(i) for i in range(12)]\n",
    "index_m = pd.MultiIndex.from_product([statistics_m, MFCCs])\n",
    "MFCC.columns = index_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create some helper functions first to visualize and view the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from ipywidgets import interact, interactive, IntSlider, Layout\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "def get_data(data):\n",
    "    if data == 'rythm':\n",
    "        return rythm\n",
    "    elif data == 'chroma':\n",
    "        return chroma\n",
    "    elif data == 'MFCC':\n",
    "        return MFCC\n",
    "\n",
    "def check_stats(stat, tail_head, scale, data):\n",
    "    data = get_data(data)\n",
    "    if stat == 'full' and tail_head == 'head':\n",
    "        if scale == 'standardization':\n",
    "            sc = StandardScaler()\n",
    "            print(pd.DataFrame(sc.fit_transform(data)).head(n = 10))\n",
    "        else:    \n",
    "            print(data.head(n = 10))\n",
    "    elif stat == 'full' and tail_head == 'tail':\n",
    "        if scale == 'standardization':\n",
    "            sc = StandardScaler()\n",
    "            print(pd.DataFrame(sc.fit_transform(data)).tail(n = 10))\n",
    "        else:  \n",
    "            print(rythm.tail(n = 10))\n",
    "    elif tail_head == 'head':\n",
    "        if scale == 'standardization':\n",
    "            sc = StandardScaler()\n",
    "            print(pd.DataFrame(sc.fit_transform(data[stat])).head(n = 10))\n",
    "        else:  \n",
    "            print(data[stat].head(n = 10))\n",
    "    else:\n",
    "        if scale == 'standardization':\n",
    "            sc = StandardScaler()\n",
    "            print(pd.DataFrame(sc.fit_transform(data[stat])).tail(n = 10))\n",
    "        else:  \n",
    "            print(data[stat].tail(n = 10))\n",
    "            \n",
    "def plot_stats(stat, type, scale, data):\n",
    "    sns.set_color_codes()\n",
    "    data_df = get_data(data)\n",
    "    melted_data = data_df[stat].melt(var_name=data, value_name='value')\n",
    "    if scale == 'standardization':\n",
    "        sc = StandardScaler()\n",
    "        melted_data = pd.DataFrame(sc.fit_transform(data_df[stat])).melt(var_name=data, value_name='value')\n",
    "        \n",
    "    g = sns.FacetGrid(melted_data, col=data, col_wrap=4)\n",
    "    \n",
    "    if type == 'box':\n",
    "        g.map(sns.boxplot, \"value\") \n",
    "    elif type == 'dist':\n",
    "        g.map(sns.distplot, \"value\", rug=True)\n",
    "    else:\n",
    "        g.map(sns.distplot, \"value\", bins=10, kde=False, rug=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Rythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c412c1cb10e461caed263e57a1733d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='stat', options=('mean', 'median', 'variance', 'kurtosis', 'skewnes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl=interactive(check_stats,\n",
    "               stat=['mean', 'median', 'variance', 'kurtosis', 'skewness', 'min', 'max','full'], \n",
    "               tail_head = ['tail', 'head'],\n",
    "               scale=['standardization', 'None'],\n",
    "               data = widgets.fixed('rythm'))\n",
    "display(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can view the first and last ten rows of the data with the interactive cell above. Seems like there is a lot of variation between columns.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a32b6aca2b242c2832edbf657f54d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='stat', options=('mean', 'median', 'variance', 'kurtosis', 'skewnes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl=interactive(plot_stats, \n",
    "               stat=['mean', 'median', 'variance', 'kurtosis', 'skewness', 'min', 'max'], \n",
    "               type=['box', 'dist', 'hist'], \n",
    "               scale=['standardization', 'None'],\n",
    "               data = widgets.fixed('rythm'))\n",
    "    \n",
    "display(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It seems like scaling the rythm part of the data since the range of the data is over 10^3 so we need to even out the distributions here. Espescially the 24th rythm column for all the statistics show some abnormal values in comparison to other values. These can be seen clearly from the plots with different settings.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087029c99d6c4de2ba01db1668378dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='stat', options=('mean', 'std', 'min', 'max', 'full'), value='mean'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl=interactive(check_stats, \n",
    "               stat=['mean', 'std', 'min', 'max','full'], \n",
    "               tail_head = ['tail', 'head'],\n",
    "               scale=['standardization', 'None'],\n",
    "               data = widgets.fixed('chroma'));\n",
    "display(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The chroma part looks tolerable based on the tail and head of the data. Lets explore some more with plots.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c40ddb75a8c41029c2baa3201c59be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='stat', options=('mean', 'std', 'min', 'max'), value='mean'), Dropd…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl=interactive(plot_stats, \n",
    "               stat=['mean', 'std', 'min', 'max'], \n",
    "               type=['box', 'dist', 'hist'], \n",
    "               scale=['standardization', 'None'],\n",
    "               data = widgets.fixed('chroma'));\n",
    "display(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can tell from the histograms that the chroma part doesn't contain any abnormalities. Next up is the MFCC part of the data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127e8fe95a7d4d37887b10f8b8af1943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='stat', options=('mean', 'std', 'min', 'max', 'full'), value='mean'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl=interactive(check_stats, \n",
    "               stat=['mean', 'std', 'min', 'max','full'], \n",
    "               tail_head = ['tail', 'head'],\n",
    "               scale=['standardization', 'None'],\n",
    "               data = widgets.fixed('MFCC'));\n",
    "display(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There are some anomalies in the first four columns of the means of the MFCC-data. We will drop those columns from before fitting the models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6fb547a7bc483c96b5c8b7f72299da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='stat', options=('mean', 'std', 'min', 'max'), value='mean'), Dropd…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl=interactive(plot_stats, \n",
    "               stat=['mean', 'std', 'min', 'max'], \n",
    "               type=['box', 'hist'],  \n",
    "               scale=['standardization', 'None'],\n",
    "               data = 'MFCC');\n",
    "display(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methods and Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Preprocessing  and Scaling\n",
    "*Based on the analysis plots of the data we decide to remove some features from the data before we continue further. From the MFCC part of the data the first four mean values will be dropped. The decision is based on the plots above.*  \n",
    "  \n",
    "*Some methods perform better when the data is scaled and some others do not. For example SVM's usually require scaling and we noticed a significant increase in accuracy when the data was scaled. Generally standardisizing all values will reduce the distortion due to exceptionally high values and make some algorithms converge faster. We will standardize our data mainly for the PCA dimensionality reduction and SVM models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train)\n",
    "\n",
    "train_scaled = pd.DataFrame(scaler.transform(train))\n",
    "test_scaled = pd.DataFrame(scaler.transform(test))\n",
    "\n",
    "# Rythm part\n",
    "rythm_train = train_scaled.iloc[:,0:168].copy()\n",
    "rythm_test = test_scaled.iloc[:,0:168].copy()\n",
    "statistics_r = ['mean', 'median', 'variance', 'kurtosis', 'skewness', 'min', 'max']\n",
    "bands = [\"band_{}\".format(i) for i in range(24)]\n",
    "index_r = pd.MultiIndex.from_product([statistics_r, bands])\n",
    "rythm_train.columns = index_r\n",
    "rythm_test.columns = index_r\n",
    "\n",
    "# Chroma part\n",
    "chroma_train = train_scaled.iloc[:,168:216].copy()\n",
    "chroma_test = test_scaled.iloc[:,168:216].copy()\n",
    "statistics_c = ['mean', 'std', 'min', 'max']\n",
    "pitches = [\"pitch_{}\".format(i) for i in range(12)]\n",
    "index_c = pd.MultiIndex.from_product([statistics_c, pitches])\n",
    "chroma_train.columns = index_c\n",
    "chroma_test.columns = index_c\n",
    "\n",
    "# MFCC part\n",
    "MFCC_train = train_scaled.iloc[:,216:].copy()\n",
    "MFCC_test = test_scaled.iloc[:,216:].copy()\n",
    "statistics_m = ['mean', 'std', 'min', 'max']\n",
    "MFCCs = [\"MFCC_{}\".format(i) for i in range(12)]\n",
    "index_m = pd.MultiIndex.from_product([statistics_m, MFCCs])\n",
    "MFCC_train.columns = index_m\n",
    "MFCC_test.columns = index_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MFCC_test.columns = [''.join(col) for col in MFCC_test.columns]\n",
    "MFCC_test.drop(columns = ['meanMFCC_0', 'meanMFCC_1', 'meanMFCC_2', 'meanMFCC_3'], axis=1, inplace=True)\n",
    "MFCC_train.columns = [''.join(col) for col in MFCC_train.columns]\n",
    "MFCC_train.drop(columns = ['meanMFCC_0', 'meanMFCC_1', 'meanMFCC_2', 'meanMFCC_3'], axis=1, inplace=True)\n",
    "\n",
    "rythm_train.columns = [''.join(col) for col in rythm_train.columns]\n",
    "rythm_test.columns = [''.join(col) for col in rythm_test.columns]\n",
    "\n",
    "chroma_train.columns = [''.join(col) for col in chroma_train.columns]\n",
    "chroma_test.columns = [''.join(col) for col in chroma_test.columns]\n",
    "\n",
    "train_processed = pd.concat([rythm_train, chroma_train, MFCC_train], axis=1)\n",
    "test_processed = pd.concat([rythm_test, chroma_test, MFCC_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feature Engineering  \n",
    "**Installation:**``pip3 install prince``  \n",
    "  \n",
    "*We will use Principle Component Analysis (PCA) to reduce some redundancy from the data. A package called ``prince`` is used for this. We will reduce the dimensionality of the data to 240 PCA-features. Reducing the features will make the model fitting significantly faster and hopefully increase the accuracy of the models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prince\n",
    "pca = prince.PCA(\n",
    "        n_components=240,\n",
    "        n_iter=3,\n",
    "        rescale_with_mean=True,\n",
    "        rescale_with_std=True,\n",
    "        copy=True,\n",
    "        engine='auto',\n",
    "        random_state=42\n",
    ")\n",
    "pca = pca.fit(train_processed)\n",
    "pca_train = pca.transform(train_processed)\n",
    "pca_test = pca.transform(test_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Helper Functions for Writing Csvs and Plotting\n",
    "*The first function is used to plot the the labels of the predicted model and training labels as a histogram. The training labels are plotted on top of the predicted labels so it helps us to see the similarities in the class distributions. The second function is used to write the csv's to the right format for kaggle competition.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "def plot_hists(predictions, train_labels=train_labels):\n",
    "    cnt = collections.Counter()\n",
    "    for label in predictions:\n",
    "        cnt[label] += 1\n",
    "\n",
    "    cnt2 = collections.Counter()\n",
    "    for label in train_labels[0]:\n",
    "        cnt2[label] += 1\n",
    "\n",
    "    print(\"prediction labels:\", cnt)\n",
    "    print(\"vs.\")\n",
    "    print(\"Train labels:\", cnt2)\n",
    "\n",
    "    y = list(cnt.values())\n",
    "    x = list(cnt.keys())\n",
    "    y2 = list(cnt2.values())\n",
    "    x2 = list(cnt2.keys())\n",
    "    plt.bar(x, y, width=1.0, color='g')\n",
    "    plt.bar(x2, y2, width=1.0, color='r', alpha=0.5)\n",
    "    \n",
    "def CSV(predictions, name):\n",
    "    pred_df = pd.DataFrame({'Sample_label':predictions})\n",
    "    pred_df.index += 1\n",
    "    pred_df.to_csv(name, \n",
    "                   sep=',',  \n",
    "                   header=True, \n",
    "                   index=True, \n",
    "                   index_label='Sample_id', \n",
    "                   mode='w', \n",
    "                   decimal='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Fitting the models\n",
    "*Methods we used in the accuracy competition were: Support Vector Machines with grid search for best parameters. Other method that was used was gradient boosting called XGBoost.*\n",
    "  \n",
    "*Grid search was used for the first two models to fine tune the hyperparameters. This results in a significant increase in execution time but can improve the models significantly as well [[2](#[2])]. For XGBoost we also did some extensive hyperparameter fine tuning to find out the optimal parameter settings among the vast amount of parameters XGBoost offers. This means that we run the grid search for XGBoost several times and the execution time is therefore a lot longer than for the other models. However XGBoost can be run with multiple cores in parallel and we used that feature to lower the execution time.*  \n",
    "  \n",
    "**Note!**  \n",
    "**``PC that was used for fitting has a 4-core intel i5 processor. The running times reflect the performance of that machine``**\n",
    "\n",
    "### 3.3.1 Kernel Support Vector Machines with Grid Search For Multiclass Classification\n",
    "\n",
    "**WARNING:**  \n",
    "``Training running time: 7.182min``  \n",
    "``Prediction running time: 0.13333min``  \n",
    "\n",
    "\n",
    "**_Support Vector Machine Method:_**  \n",
    "*Support Vector Machines are a supervised machine learning technique that are based on structural risk minimization. SVM tries\n",
    "to find an optimized hyper-plane in a kernel space where training instances are linearly separable.[[3](#[3])]*\n",
    "\n",
    "*Parameter ``C`` is a penalty parameter of the error term, we want to find the optimal penalty parameter so we do not penalize too much or too little to avoid over or underfitting the model. The ``C`` parameter trades off correct classification of training examples against maximization of the decision function’s margin. For larger values of ``C``, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. A lower C will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy. In other words``C`` behaves as a regularization parameter in the SVM. [[4](#[4])]*  \n",
    "  \n",
    "*Parameter ``gamma`` is a kernel coefficient for different kernels. Intuitively, the ``gamma`` parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The ``gamma`` parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors. [[4](#[4])]*  \n",
    "\n",
    "**_Hyperparameter tuning with grid Search:_**  \n",
    "*We will use a grid search algorithm for the given parameter grid below for hyperparameter tuning. The fitting can take a long time so we use only three searches, each model is fitted two times so we get 5 x 4 x 1 x 2 x 2 = 80 fits in total. We will also use two cores in parallel for the search. We will not use a ``linear`` kernel in the search because the execution time would skyrocket.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scoring = {'logloss':'neg_log_loss', 'Accuracy': make_scorer(accuracy_score)}\n",
    "param_grid = {'C':[10, 20, 65, 90, 102], \n",
    "              'gamma':[0.001, 0.1, 0.0001, 0.01], \n",
    "              'kernel':['rbf']}\n",
    "\n",
    "grid_svc = GridSearchCV(SVC(probability=True), \n",
    "                        param_grid,\n",
    "                        scoring=scoring, \n",
    "                        cv=2, \n",
    "                        refit='logloss',\n",
    "                        return_train_score=True,\n",
    "                        n_jobs=2,\n",
    "                        verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_start = time.perf_counter()\n",
    "grid_svc.fit(pca_train, train_labels)\n",
    "training_end = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:  SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False) \n",
      "\n",
      "Time consumed for training: 7.182 mins \n",
      "\n",
      "Time consumed for prediction: 0.13333 mins \n",
      "\n",
      "prediction labels: Counter({1: 4178, 2: 896, 3: 506, 4: 360, 6: 221, 8: 142, 5: 123, 9: 81, 10: 21, 7: 16})\n",
      "vs.\n",
      "Train labels: Counter({1: 2178, 2: 618, 3: 326, 6: 260, 4: 253, 5: 214, 8: 195, 7: 141, 9: 92, 10: 86})\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction_start = time.perf_counter()\n",
    "predictions_svm = grid_svc.predict(pca_test)\n",
    "prediction_end = time.perf_counter()\n",
    "\n",
    "svm_train_time = training_end-training_start\n",
    "svm_prediction_time = prediction_end-prediction_start\n",
    "\n",
    "print(\"Best estimator: \", grid_svc.best_estimator_, \"\\n\")\n",
    "print(\"Time consumed for training: %4.3f mins\" % (svm_train_time/60), \"\\n\")\n",
    "print(\"Time consumed for prediction: %6.5f mins\" % (svm_prediction_time/60), \"\\n\")\n",
    "plot_hists(predictions_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 XGBoost Gradient Boosting with Grid Search and Hyperparameter Fine Tuning\n",
    "\n",
    "**WARNING:**  \n",
    "``Training running time: 28.77978min + 9.68299min + 11.39813min + 38.42624min +21.957011min = 1.83hour``  \n",
    "``Prediction running time: 0.00571min``   \n",
    "  \n",
    "**Installation of the package:** ```pip3 install xgboost```  \n",
    "  \n",
    "**Or use this stackoverflow answer to install the package (windows).**  \n",
    "https://stackoverflow.com/questions/33749735/how-to-install-xgboost-package-in-python-windows-platform/39811079#39811079\n",
    "\n",
    "**_XGBoosting method:_**  \n",
    "*Boosting is an Ensemble technique where a different model is trained with same data for each iteration. Each sample is assigned a different weight in each iteration. XGBoost can be prone to overfitting but with the tuning of hyperparameters this overfitting can be avoided.[[5](#[5])]*  \n",
    "  \n",
    "*XGBoost, short for eXtreme Gradient Boosting, is an advanced machine learning model implemented from gradient boosted decision trees. It is designed for speed and performance, and is powerful enough to deal with irregular data. XGBoost uses multiple parameters which have to be fine tuned to optimize the model, which is why improving a XGBoost model is difficult.[[6](#[6])]*  \n",
    "  \n",
    "*The parameters are divided into General Parameters, Booster Parameters and Learning Task Parameters. In our solution we have used a total of 11 parameters to optimize our model. ``Learning_rate`` makes the model more sturdy by shrinking the weights on each step, ``b_estimators`` denotes the number of sequential trees to be modeled and ``max_depth`` is used to control over-fitting. Higher values allow the model to learn relations that are specific to a particular sample. Min_child_weight is also used to control over-fitting. In this case higher values prevent the model from using relations that are specific to a particular sample. Parameter ``gamma`` is used to make the algorithm conservative by specifying the minimum loss reduction required to make a split, ``subsample`` gives the fraction of observations to be randomly selected for each tree and ``colsample_bytree`` indicates the fraction of columns to be random samples for each tree. Parameter ``objective`` defines the loss function that is to be minimized, ``nthread`` is used for parallel processing denoting the number of cores to be used, ``scale_pos_weight`` can be used to help in faster convergence in case of high class imbalance and seed is the random number seed used to generate reproducible “random” results.[[6](#[6])]*\n",
    "\n",
    "**_Fine Tuning Hyperparameters with Multiple Grid searches_**:   \n",
    "*We will fine tune the hyperparameters with multiple grid searches for the model. The fitting can take a long time so we use only three searches, each model is fitted three times so we get several different fits. We will use also four cores in parallel to make the algorithms run faster.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Parameter tuning for XGBoost\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "\n",
    "xgb_classifier = XGBClassifier(learning_rate =0.1, \n",
    "                               n_estimators=140,\n",
    "                               max_depth=5,\n",
    "                               min_child_weight=1, \n",
    "                               gamma=0, \n",
    "                               subsample=0.8, \n",
    "                               colsample_bytree=0.8,\n",
    "                               objective='reg:gamma', \n",
    "                               nthread=4, \n",
    "                               scale_pos_weight=1, \n",
    "                               seed=27)\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator = xgb_classifier, \n",
    "                        param_grid = param_test1, \n",
    "                        n_jobs=4,\n",
    "                        iid=False, \n",
    "                        cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_start = time.perf_counter()\n",
    "gsearch1.fit(pca_train, train_labels)\n",
    "xgb_train_end = time.perf_counter()\n",
    "xgb_training_time = xgb_train_end-xgb_train_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'min_child_weight': 3}\n",
      "0.6176612330754834\n",
      "Time consumed for training model: 28.77978 mins\n"
     ]
    }
   ],
   "source": [
    "print(gsearch1.best_params_)\n",
    "print(gsearch1.best_score_)\n",
    "print(\"Time consumed for training model: %6.5f mins\" % (xgb_training_time/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    " 'min_child_weight':[6,8,10,12]\n",
    "}\n",
    "\n",
    "xgb_classifier = XGBClassifier(learning_rate=0.1, \n",
    "                               n_estimators=140, \n",
    "                               max_depth=7,\n",
    "                               min_child_weight=5, \n",
    "                               gamma=0, \n",
    "                               subsample=0.8, \n",
    "                               colsample_bytree=0.8,\n",
    "                               objective= 'reg:gamma', \n",
    "                               nthread=4, \n",
    "                               scale_pos_weight=1,\n",
    "                               seed=27)\n",
    "\n",
    "gsearch2 = GridSearchCV(estimator = xgb_classifier, \n",
    "                        param_grid = param_test2, \n",
    "                        n_jobs=4,\n",
    "                        iid=False, \n",
    "                        cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_start = time.perf_counter()\n",
    "gsearch2.fit(pca_train, train_labels)\n",
    "xgb_train_end = time.perf_counter()\n",
    "xgb_training_time = xgb_train_end-xgb_train_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_child_weight': 12}\n",
      "0.6206667832191991\n",
      "Time consumed for training XGBoost model: 9.68299 mins\n"
     ]
    }
   ],
   "source": [
    "print(gsearch2.best_params_)\n",
    "print(gsearch2.best_score_)\n",
    "print(\"Time consumed for training XGBoost model: %6.5f mins\" % (xgb_training_time/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "xgb_classifier = XGBClassifier(learning_rate = 0.1, \n",
    "                               n_estimators=140, \n",
    "                               max_depth=7,\n",
    "                               min_child_weight=12, \n",
    "                               gamma=0, \n",
    "                               subsample=0.8, \n",
    "                               colsample_bytree=0.8,\n",
    "                               objective= 'reg:gamma', \n",
    "                               nthread=4, \n",
    "                               scale_pos_weight=1,\n",
    "                               seed=27)\n",
    "\n",
    "gsearch3 = GridSearchCV(estimator = xgb_classifier, \n",
    "                        param_grid = param_test3, \n",
    "                        n_jobs=4,\n",
    "                        iid=False, \n",
    "                        cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_start = time.perf_counter()\n",
    "gsearch3.fit(pca_train, train_labels)\n",
    "xgb_train_end = time.perf_counter()\n",
    "xgb_training_time = xgb_train_end-xgb_train_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.3}\n",
      "0.6209066470960698\n",
      "Time consumed for training model: 11.39813 mins\n"
     ]
    }
   ],
   "source": [
    "print(gsearch3.best_params_)\n",
    "print(gsearch3.best_score_)\n",
    "print(\"Time consumed for training model: %6.5f mins\" % (xgb_training_time/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "\n",
    "xgb_classifier = XGBClassifier(learning_rate=0.1, \n",
    "                               n_estimators=177, \n",
    "                               max_depth=7,\n",
    "                               min_child_weight=12, \n",
    "                               gamma=0.3, \n",
    "                               subsample=0.8, \n",
    "                               colsample_bytree=0.8,\n",
    "                               objective='reg:gamma',\n",
    "                               nthread=4, \n",
    "                               scale_pos_weight=1,\n",
    "                               seed=27)\n",
    "\n",
    "gsearch4 = GridSearchCV(estimator = xgb_classifier,\n",
    "                        param_grid = param_test4,\n",
    "                        n_jobs=4,\n",
    "                        iid=False,\n",
    "                        cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_start = time.perf_counter()\n",
    "gsearch4.fit(pca_train, train_labels)\n",
    "xgb_train_end = time.perf_counter()\n",
    "xgb_training_time = xgb_train_end-xgb_train_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.6, 'subsample': 0.7}\n",
      "0.6252332781221216\n",
      "Time consumed for training model: 38.42624 mins\n"
     ]
    }
   ],
   "source": [
    "print(gsearch4.best_params_)\n",
    "print(gsearch4.best_score_)\n",
    "print(\"Time consumed for training model: %6.5f mins\" % (xgb_training_time/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(75,90,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(75,90,5)]\n",
    "}\n",
    "\n",
    "xgb_classifier = XGBClassifier(learning_rate =0.1, \n",
    "                               n_estimators=177, \n",
    "                               max_depth=9,\n",
    "                               min_child_weight=12, \n",
    "                               gamma=0.1, \n",
    "                               subsample=0.7, \n",
    "                               colsample_bytree=0.6,\n",
    "                               objective='reg:gamma', \n",
    "                               nthread=4, \n",
    "                               scale_pos_weight=1,\n",
    "                               seed=27)\n",
    "\n",
    "gsearch5 = GridSearchCV(estimator = xgb_classifier, \n",
    "                        param_grid = param_test5, \n",
    "                        n_jobs=4,\n",
    "                        iid=False,\n",
    "                        cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_start = time.perf_counter()\n",
    "gsearch5.fit(pca_train, train_labels)\n",
    "xgb_train_end = time.perf_counter()\n",
    "xgb_training_time = xgb_train_end-xgb_train_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.85, 'subsample': 0.8}\n",
      "0.631206086070387\n",
      "Time consumed for training model: 21.95701 mins\n"
     ]
    }
   ],
   "source": [
    "print(gsearch5.best_params_)\n",
    "print(gsearch5.best_score_)\n",
    "print(\"Time consumed for training model: %6.5f mins\" % (xgb_training_time/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=0.85, gamma=0.1, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=9, min_child_weight=12, missing=None,\n",
      "       n_estimators=177, n_jobs=1, nthread=4, objective='multi:softprob',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=27, silent=True, subsample=0.85) \n",
      "\n",
      "Time consumed for training: 25.391 mins \n",
      "\n",
      "Time consumed for prediction: 0.00571 mins \n",
      "\n",
      "prediction labels: Counter({1: 4639, 2: 783, 3: 467, 4: 225, 6: 173, 5: 102, 8: 99, 9: 38, 7: 12, 10: 6})\n",
      "vs.\n",
      "Train labels: Counter({1: 2178, 2: 618, 3: 326, 6: 260, 4: 253, 5: 214, 8: 195, 7: 141, 9: 92, 10: 86})\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction_start = time.perf_counter()\n",
    "predictions_xgb =gsearch5.predict(pca_test)\n",
    "prediction_end = time.perf_counter()\n",
    "\n",
    "xgb_prediction_time = prediction_end-prediction_start\n",
    "\n",
    "print(\"Best estimator: \", gsearch5.best_estimator_, \"\\n\")\n",
    "print(\"Time consumed for training: %4.3f mins\" % (xgb_training_time/60), \"\\n\")\n",
    "print(\"Time consumed for prediction: %6.5f mins\" % (xgb_prediction_time/60), \"\\n\")\n",
    "plot_hists(predictions_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Linear Support Vector Machines for Multiclass Probability Classification\n",
    "\n",
    "**WARNING:**  \n",
    "``Training running time: 6.887min``  \n",
    "``Prediction running time: 6.897min``  \n",
    "  \n",
    "**_Linear SVMs for multiclass probability classification (OneVsRest):_**  \n",
    "*Linear SVM's support multiclass probability classification so we will use them to compare SVM's against XGBoosting in this part.*  \n",
    "  \n",
    "**Bagging technique:**  \n",
    "*We will use an Ensemble technique called Bagging (Bootstrap Aggregating) to decrease the execution time substantially. In Bagging a subset of the data is taken and then trained to a set of different models. It should reduce variance and help to avoid overfitting.[[7](#[7])]*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "n_estimators = 200\n",
    "svm_multiclass = OneVsRestClassifier(BaggingClassifier(SVC(kernel='linear', \n",
    "                                                           probability=False), \n",
    "                                                       max_samples=40/n_estimators, \n",
    "                                                       n_estimators=n_estimators))\n",
    "\n",
    "start = time.time()\n",
    "svm_multiclass.fit(pca_train, train_labels)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time consumed for bagging SVM training: 6.887 mins\n",
      "Time consumed for bagging SVC predictions: 6.897 mins\n"
     ]
    }
   ],
   "source": [
    "print(\"Time consumed for bagging SVM training: %4.3f mins\" % ((end - start)/60))\n",
    "start = time.time()\n",
    "proba = svm_multiclass.predict_proba(pca_test)\n",
    "end = time.time()\n",
    "print(\"Estimator: \", svm_multiclass, \"\\n\")\n",
    "print(\"Time consumed for bagging SVC predictions: %4.3f mins\" % ((end - start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle CSV format\n",
    "pred_df = pd.DataFrame(proba)\n",
    "pred_df.index += 1\n",
    "pred_df.columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9', 'Class_10']\n",
    "pred_df.to_csv('SVM_multiclass_pca2.csv', \n",
    "               sep=',',  \n",
    "               header=True, \n",
    "               index=True, \n",
    "               index_label='Sample_id', \n",
    "               mode='w', \n",
    "               decimal='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 XGBboost for Multiclass Probability Classification\n",
    "\n",
    "**WARNING:**  \n",
    "``Training running time: 7.502min``  \n",
    "``Prediction running time: 0.05142min``  \n",
    " \n",
    "**_XGBoosting for multiclass probability classification method (OneVsRest):_**  \n",
    "*In this solution for Multiclass Classification using XGBoost we have used the following 10 parameters: ``max_depth``, ``learning_rate (eta)``, ``silent``, ``objective``, ``num_class``, ``gamma``, ``subsample``, ``colsample_bytree``, ``n_estimators`` and ``eval_metric``. By setting objective to ``multi:softprob`` we set XGBoost to do multiclass classification. We also had to set ``num_class`` to the number of classes in our labelset. ``Eval_metric`` is used to validate data so we set it to ``mlogloss`` to give us the multiclass logloss. We also activated the silent mode so as to not print any running messages.[[6](#[6]), [8](#[8]), [9](#[9])]*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "param = {\n",
    "    'max_depth':9,\n",
    "    'min_child_weight':12,\n",
    "    'eta':0.05,\n",
    "    'silent':1,\n",
    "    'objective':'multi:softprob',\n",
    "    'num_class':10,\n",
    "    'gamma':0.1,\n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.9,\n",
    "    'n_estimators':1000,\n",
    "    'eval_metric':'mlogloss'}  \n",
    "\n",
    "labels_multi = LabelEncoder()\n",
    "y_train = labels_multi.fit_transform(train_labels.values)\n",
    "X_train = pca_train.values\n",
    "\n",
    "xgb_classifier = XGBClassifier(**param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_start = time.perf_counter()\n",
    "xgb_classifier.fit(X_train, y_train, eval_metric='mlogloss')\n",
    "xgb_train_end = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator:  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=0.9, eta=0.05, eval_metric='mlogloss', gamma=0.1,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=9,\n",
      "       min_child_weight=12, missing=None, n_estimators=1000, n_jobs=1,\n",
      "       nthread=None, num_class=10, objective='multi:softprob',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=1, subsample=0.8) \n",
      "\n",
      "Time consumed for training: 7.502 mins \n",
      "\n",
      "Time consumed for prediction: 0.05142 mins \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_start = time.perf_counter()\n",
    "preds = xgb_classifier.predict_proba(pca_test.values)\n",
    "prediction_end = time.perf_counter()\n",
    "\n",
    "xgb_training_time = xgb_train_end-xgb_train_start\n",
    "xgb_prediction_time = prediction_end-prediction_start\n",
    "\n",
    "print(\"Estimator: \", xgb_classifier, \"\\n\")\n",
    "print(\"Time consumed for training: %4.3f mins\" % (xgb_training_time/60), \"\\n\")\n",
    "print(\"Time consumed for prediction: %6.5f mins\" % (xgb_prediction_time/60), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle CSV format\n",
    "pred_df = pd.DataFrame(preds)\n",
    "pred_df.index += 1\n",
    "pred_df.columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9', 'Class_10']\n",
    "pred_df.to_csv('XGBoost_multiclass_pca.csv', \n",
    "               sep=',',  \n",
    "               header=True, \n",
    "               index=True, \n",
    "               index_label='Sample_id', \n",
    "               mode='w', \n",
    "               decimal='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Evaluation methodology  \n",
    "*We used K-Fold Cross Validation for model evaluation. K-Fold Cross Evaluation is an evaluation method used to estimate the skill of the model on new data.*  \n",
    "\n",
    "  \n",
    "*The general procedure is as follows:*\n",
    "\n",
    " 1. Shuffle the dataset randomly\n",
    " 2. Split the dataset into k groups\n",
    " 3. For each unique group:\n",
    "     1. Take the group as a hold out or test data set\n",
    "     2. Take the remaining groups as a training data set\n",
    "     3. Fit a model on the training set and evaluate it on the test set\n",
    "     4. Retain the evaluation score and discard the model\n",
    " 4. Summarize the skill of the model using the sample of model evaluation scores\n",
    " [[10](#[10])]\n",
    "\n",
    "**_Performance Metrics used:_**\n",
    "  \n",
    "*__Accuracy for multiclass classification:__*  \n",
    "*Classification accuracy is the number of correct predictions made as a ratio of all predictions made. This is the most common evaluation metric for classification problems, it is also the most misused. It is really only suitable when there are an equal number of observations in each class (which is rarely the case) and that all predictions and prediction errors are equally important, which is often not the case.* [[13](#[13])]  \n",
    "  \n",
    "*__Log loss for multiclass probabilities:__*  \n",
    "*Log Loss is the most important classification metric based on probabilities. It's hard to interpret raw log-loss values, but log-loss is still a good metric for comparing models. For any given problem, a lower log-loss value means better predictions whereas for accuracy a larger value means a better prediction.*\n",
    "\n",
    "### 3.4.1 Kernel SVM Model Evaluation  \n",
    "  \n",
    "**WARNING:**  \n",
    "``3-Fold cross validation running time: 36.83511 seconds``\n",
    "\n",
    "**``Accuracy score mean: 0.6520697773909084``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time consumed for 3-Fold cross validation: 36.83511 seconds \n",
      "\n",
      "accuracy mean:  0.6520697773909084\n",
      "accuracy std:  0.006388245623531938\n"
     ]
    }
   ],
   "source": [
    "cross_val_start = time.perf_counter()\n",
    "accuracies_svm = cross_val_score(estimator = grid_svc.best_estimator_, X=pca_train, y=train_labels, cv=3)\n",
    "cross_val_end = time.perf_counter()\n",
    "\n",
    "svm_cross_val_time = cross_val_end-cross_val_start\n",
    "print(\"Time consumed for 3-Fold cross validation: %6.5f seconds\" % (svm_cross_val_time), \"\\n\")\n",
    "print('accuracy mean: ', accuracies_svm.mean())\n",
    "print('accuracy std: ', accuracies_svm.std())\n",
    "# CSV(predictions_svm, 'svm_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 XGBoosting Model Evaluation  \n",
    "  \n",
    "**WARNING:**  \n",
    "``3-Fold cross validation running time: 82.22754 seconds``\n",
    "\n",
    "**``Accuracy score mean: 0.6163100317761705``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time consumed for 3-Fold cross validation: 82.22754 seconds \n",
      "\n",
      "accuracy mean:  0.6163100317761705\n",
      "accuracy std:  0.0052570544007903355\n"
     ]
    }
   ],
   "source": [
    "cross_val_start = time.perf_counter()\n",
    "accuracies_xgb = cross_val_score(estimator = gsearch5.best_estimator_, X=pca_train, y=train_labels, cv=3)\n",
    "cross_val_end = time.perf_counter()\n",
    "\n",
    "xgb_cross_val_time = cross_val_end-cross_val_start\n",
    "print(\"Time consumed for 3-Fold cross validation: %6.5f seconds\" % (xgb_cross_val_time), \"\\n\")\n",
    "print('accuracy mean: ', accuracies_xgb.mean())\n",
    "print('accuracy std: ', accuracies_xgb.std())\n",
    "# CSV(predictions_xgb, 'xgb_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Linear SVM Multiclass Probability Model Evaluation\n",
    "  \n",
    "**WARNING:**  \n",
    "``3-Fold cross validation running time: 9.08431min``  \n",
    "  \n",
    "**``Log Loss: -1.4261799781067401``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_start = time.perf_counter()\n",
    "log_loss_svm = cross_val_score(svm_multiclass, pca_train, train_labels, scoring='neg_log_loss', cv=3)\n",
    "cross_val_end = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time consumed for 3-Fold cross validation: 12.25361 mins \n",
      "\n",
      "log loss mean:  -1.4261799781067401\n",
      "log loss std:  0.019569384136041576\n"
     ]
    }
   ],
   "source": [
    "svm_cross_val_time = cross_val_end-cross_val_start\n",
    "print(\"Time consumed for 3-Fold cross validation: %6.5f mins\" % ((svm_cross_val_time)/60), \"\\n\")\n",
    "print('log loss mean: ', log_loss_svm.mean())\n",
    "print('log loss std: ', log_loss_svm.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 XGBboost for Multiclass Probability Model Evaluation\n",
    "  \n",
    "**WARNING:**  \n",
    "``3-Fold cross validation running time: 12.25361min``\n",
    "\n",
    "**``Log Loss: -1.2879083790893804``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_start = time.perf_counter()\n",
    "log_loss_xgb = cross_val_score(xgb_classifier, pca_train, train_labels, scoring='neg_log_loss', cv=3)\n",
    "cross_val_end = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time consumed for 3-Fold cross validation: 12.25361 mins \n",
      "\n",
      "log loss mean:  -1.2879083790893804\n",
      "log loss std:  0.026054138879414608\n"
     ]
    }
   ],
   "source": [
    "xgb_cross_val_time = cross_val_end-cross_val_start\n",
    "print(\"Time consumed for 3-Fold cross validation: %6.5f mins\" % ((xgb_cross_val_time)/60), \"\\n\")\n",
    "print('log loss mean: ', log_loss_xgb.mean())\n",
    "print('log loss std: ', log_loss_xgb.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Confusion matrices  \n",
    "*Here are the confusion matrices for the models used in the accuracy competition.*\n",
    "#### 4.1.1 SVM  \n",
    "``Running time: 35.45342659950256sec``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 35.45342659950256\n",
      "[[1950  120   20   16   16   36    4   13    1    2]\n",
      " [ 194  370   22   15    4    8    0    3    2    0]\n",
      " [  61   26  215    2    1   12    0    0    9    0]\n",
      " [  92   21    3  110    4   16    3    1    1    2]\n",
      " [ 138   11    8    6   33    7    3    3    5    0]\n",
      " [ 118   11   19   12    6   86    0    4    4    0]\n",
      " [  94   13    4    7    4    6    5    3    4    1]\n",
      " [ 131    2    1    0    3    5    1   50    0    2]\n",
      " [  24    7   17    1    8   11    0    1   23    0]\n",
      " [  55    2    2    8    3    3    2    5    3    3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "start = time.time()\n",
    "y_pred = cross_val_predict(estimator = grid_svc.best_estimator_, X=pca_train, y=train_labels, cv=3)\n",
    "conf_mat = confusion_matrix(train_labels, y_pred)\n",
    "end = time.time()\n",
    "print(\"Running time:\", (end-start))\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 XGboost  \n",
    "``Running time: 78.00097107887268sec``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 78.00097107887268\n",
      "[[2011   83   19   15    7   23    0   17    3    0]\n",
      " [ 285  299   19    5    3    4    0    2    1    0]\n",
      " [  83   32  197    1    1    7    0    0    3    2]\n",
      " [ 138   26    1   72    4    7    1    2    1    1]\n",
      " [ 151    9    6    6   23    9    4    4    2    0]\n",
      " [ 157   14   26    5    6   46    0    4    1    1]\n",
      " [ 100   16    5    2    5    5    2    2    2    2]\n",
      " [ 152    1    1    3    4    1    0   33    0    0]\n",
      " [  39   13   20    2    4    6    1    1    6    0]\n",
      " [  68    2    4    8    1    1    1    1    0    0]]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "y_pred = cross_val_predict(estimator = gsearch5.best_estimator_, X=pca_train, y=train_labels, cv=3)\n",
    "conf_mat = confusion_matrix(train_labels, y_pred)\n",
    "end = time.time()\n",
    "print(\"Running time:\", (end-start))\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Scores\n",
    "#### 4.2.1 Accuracy scores\n",
    "*SVMs performed better in the accuracy competition than XGBoosting. Below are the scores we acquired using the training data. According to the training data results the SVMs resulted in a score of 4 percent units higher than XGBoosting. This did not corresponded with kaggle results, where we got very similar scores to our SVM model but better scores on XGB model. This indicates that our SVM model works as well on the test data as on the training data, however the same cannot be said for XGBoost model.*  \n",
    "  \n",
    "**_Train data:_**  \n",
    "``SVM: 0.6520697773909084``  \n",
    "``XGBoost: 0.6163100317761705``  \n",
    "\n",
    "*(best kaggle accuracy for SVM model: 0.66479)*  \n",
    "*(best kaggle accuracy for XGBoost model: 0.66428)* \n",
    "  \n",
    "#### 4.2.2 Logloss scores\n",
    "*In the log loss competition we got very different values using the training data compared to the performance score on kaggle. Unfortunately we could not find out how to reproduce the results on kaggle, a reason might be that we are using the wrong function. According to our calculations XGBoost performed worse on the training data than SVMs, but on kaggle SVMs scored a much better score on log loss. We did however notice, that XGBoost performed better in both competitions on kaggle when the data was not standardized before fitting the model.*  \n",
    "  \n",
    "  \n",
    "**_Train data:_**  \n",
    "``SVM: -1.4261799781067401``  \n",
    "``XGBoost: -1.2879083790893804``\n",
    "  \n",
    "*(best kaggle logloss for SVM model: 0.18888)*  \n",
    "*(best kaggle logloss for XGBoost model: 0.19895)*\n",
    "\n",
    "### 4.3 Running times\n",
    "*We also decided to compare training running times and prediction running times to find the best performing model. Especially in the accuracy competition it is clear that SVMs are much faster than XGBoosting models, with 7.182 minutes and 1.83 hours respectively. This was due to the extensive hyperparameter fine tuning for the XGBoost model. There was not much of a difference in performance when comparing SVMs and XGBoosting for multiclass probability models, since the same hyperparameters were used for these models as in the accuracy competition.*  \n",
    "  \n",
    "**_Training running time:_**  \n",
    "``Kernel SVM Model Evaluation: 7.182 min``  \n",
    "``XGBoosting Model Evaluation: 1.83 hours``  \n",
    "``Linear SVM Multiclass Probability Model Evaluation: 6.887 min``  \n",
    "``XGBoost for Multiclass Probability Model Evaluation: 7.502 min``  \n",
    "\n",
    "*Whereas SVMs are faster in training, we discovered that XGBoosting models perform better when doing predictions. The difference is not very big, but XGBoosting models outperformed SVMs in both the accuracy and log loss competition when it comes to prediction running time.*  \n",
    "  \n",
    "**_Prediction running time:_**  \n",
    "``Kernel SVM Model Evaluation: 36.83511 sec``  \n",
    "``XGBoosting Model Evaluation: 82.22754 sec``  \n",
    "``Linear SVM Multiclass Probability Model Evaluation: 6.897 min``  \n",
    "``XGBoost for Multiclass Probability Model Evaluation: 0.05142 min``  \n",
    "\n",
    "*We also discovered that using PCA before fitting the models worsened the overall scores of the models, but made the fitting processes faster, which is why we in the end decided to keep the PCA processing in our solution.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion/Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After analysing and ranking the scores it is apparent that Support Vector Machines perform better in both competitions and also outperforms XGBoosting in training running time. XGBoosting only outperforms Support Vector Machines in prediction running time and in the accuracy and log loss competitions. XGboost however performs significantly better on non-scaled data but they still did not reach the performance of the SVM models. From this we can conclude that SVMs work better on this particular set of imbalanced data points.*  \n",
    "  \n",
    "*Our results do not follow the general consensus found from literature. XGBoost is usually one of the top methods for any classification problems but here we show that for an imbalanced data set such as the data used in this project will not fit as well the XGBoost model as it does in a more traditional SVM model.*  \n",
    "  \n",
    "*This is a clear indication that XGBoost is not always the superior method for multiclass classification problems even though the performance of the method is well shown in literature and in multiple Kaggle competitions.[[11](#[11]), [12](#[12])]*  \n",
    "  \n",
    "**_About model evaluation metrics:_**  \n",
    "*Accuracy is a metric based on a threshold and a qualitative understanding of error. This measure is used when we want a model to minimise the number of errors. Hence, the metric is usual in many direct applications of classifiers. Logloss (cross entropy) is a metric based on a probabilistic understanding of error, i.e. measuring the deviation from the true probability.*  \n",
    "  \n",
    "*As we pointed out in chapter 2.1 the training data is highly imbalanced with most data points belonging to a class labeled as 1. Among other things this can cause problems in the accuracy scores, where the accuracy measure only reflects the imbalance in the dataset by giving the majority class an advantage if the accuracy is used for guiding the learning process. To get a better fitting model a dataset where the classes are represented equally would give better results.*  \n",
    "  \n",
    "*The same applies to LogLoss since the metric does use the class predictions from the decision function, but weights each type of misclassification equally. This makes the metric lose it's precision and for that it is not a robust metric for class imbalance. Log loss should incorporate misprediction costs to account for class imbalance.*\n",
    "  \n",
    "*To combat the imbalance issue a larger dataset could be a solution as it might expose a different and more balanced perspective on the classes. This could be done by collecting more data or generating synthetic samples. If we had more resources it would also be good practice to try out even more algorithms. In this project we played around with Random Forests and Decision Trees as well but quickly found out that SVMs and XGBoosts outperformed them. If collecting more data isn’t possible, we could try using Penalized Models instead in order to force the models to pay more attention to the minority classes.*\n",
    " \n",
    "*To conclude this report, it is safe to say that highly popular machine learning methods are not always the best option. In this report we showed that the more traditional SVM model outperformed XGBoost when using a highly inblanced training dataset. It is always worthwhile to try out a few different algorithms to get a glimpse of what might or might not work with that specific dataset.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### [1]   \n",
    "*https://www.toptal.com/algorithms/shazam-it-music-processing-fingerprinting-and-recognition*  \n",
    "##### [2]  \n",
    "*https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search*  \n",
    "##### [3]   \n",
    "http://www.svms.org/srm/  \n",
    "##### [4]   \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC  \n",
    "##### [5]   \n",
    "https://datastoriesweb.wordpress.com/category/classification/  \n",
    "##### [6]  \n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/  \n",
    "##### [7]  \n",
    "https://en.wikipedia.org/wiki/Bootstrap_aggregating  \n",
    "##### [8]  \n",
    "https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters  \n",
    "##### [9]  \n",
    "https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/  \n",
    "##### [10]    \n",
    "https://machinelearningmastery.com/k-fold-cross-validation/  \n",
    "#### [11]  \n",
    "https://medium.com/syncedreview/tree-boosting-with-xgboost-why-does-xgboost-win-every-machine-learning-competition-ca8034c0b283 \n",
    "#### [12]\n",
    "https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335\n",
    "#### [13]\n",
    "https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Appendix\n",
    "  \n",
    "*We also tried Multiple Factor Analysis for dimensionality reduction, which did not turn out to be as efficient as PCA in term of the scores. We tried also XGBoosting without PCA and scaling the data, but the running times were even longer than with PCA so we decided to not include those results in the models discussed even though XGBoost performed almost as well as the best SVMs when the data was not scaled. Below is the code for Multiple Factor Analysis if one wants to use that for dimensionality reduction. XGBoost can also be run without scaling the data by just commenting out the scalers in the code above and make sure that the models are fitted to right data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train)\n",
    "\n",
    "# Hack\n",
    "scaler2 = StandardScaler()\n",
    "test_scaled = pd.DataFrame(scaler2.fit_transform(test))\n",
    "\n",
    "train_scaled = pd.DataFrame(scaler.transform(train))\n",
    "# test_scaled = pd.DataFrame(scaler.transform(test))\n",
    "\n",
    "# Rythm part\n",
    "rythm_train = train_scaled.iloc[:,0:168].copy()\n",
    "rythm_test = test_scaled.iloc[:,0:168].copy()\n",
    "statistics_r = ['mean', 'median', 'variance', 'kurtosis', 'skewness', 'min', 'max']\n",
    "bands = [\"band_{}\".format(i) for i in range(24)]\n",
    "index_r = pd.MultiIndex.from_product([statistics_r, bands])\n",
    "rythm_train.columns = index_r\n",
    "rythm_test.columns = index_r\n",
    "\n",
    "# Chroma part\n",
    "chroma_train = train_scaled.iloc[:,168:216].copy()\n",
    "chroma_test = test_scaled.iloc[:,168:216].copy()\n",
    "statistics_c = ['mean', 'std', 'min', 'max']\n",
    "pitches = [\"pitch_{}\".format(i) for i in range(12)]\n",
    "index_c = pd.MultiIndex.from_product([statistics_c, pitches])\n",
    "chroma_train.columns = index_c\n",
    "chroma_test.columns = index_c\n",
    "\n",
    "# MFCC part\n",
    "MFCC_train = train_scaled.iloc[:,216:].copy()\n",
    "MFCC_test = test_scaled.iloc[:,216:].copy()\n",
    "statistics_m = ['mean', 'std', 'min', 'max']\n",
    "MFCCs = [\"MFCC_{}\".format(i) for i in range(12)]\n",
    "index_m = pd.MultiIndex.from_product([statistics_m, MFCCs])\n",
    "MFCC_train.columns = index_m\n",
    "MFCC_test.columns = index_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MFCC_test.columns = [''.join(col) for col in MFCC_test.columns]\n",
    "MFCC_test.drop(columns = ['meanMFCC_0', 'meanMFCC_1', 'meanMFCC_2', 'meanMFCC_3'], axis=1, inplace=True)\n",
    "MFCC_train.columns = [''.join(col) for col in MFCC_train.columns]\n",
    "MFCC_train.drop(columns = ['meanMFCC_0', 'meanMFCC_1', 'meanMFCC_2', 'meanMFCC_3'], axis=1, inplace=True)\n",
    "\n",
    "rythm_train.columns = [''.join(col) for col in rythm_train.columns]\n",
    "rythm_train.drop(columns = ['meanband_23', 'medianband_23', 'varianceband_23', 'kurtosisband_23', 'minband_23', 'maxband_23'], \n",
    "           axis=1, \n",
    "           inplace=True)\n",
    "\n",
    "rythm_test.columns = [''.join(col) for col in rythm_test.columns]\n",
    "rythm_test.drop(columns = ['meanband_23', 'medianband_23', 'varianceband_23', 'kurtosisband_23', 'minband_23', 'maxband_23'], \n",
    "           axis=1, \n",
    "           inplace=True)\n",
    "\n",
    "chroma_train.columns = [''.join(col) for col in chroma_train.columns]\n",
    "chroma_test.columns = [''.join(col) for col in chroma_test.columns]\n",
    "\n",
    "train_processed = pd.concat([rythm_train, chroma_train, MFCC_train], axis=1)\n",
    "test_processed = pd.concat([rythm_test, chroma_test, MFCC_test], axis=1)\n",
    "\n",
    "groups = {\n",
    "    'mean_rythm':['meanband_{}'.format(i) for i in range(23)],\n",
    "    'median_rythm':['medianband_{}'.format(i) for i in range(23)],\n",
    "    'variance_rythm':['varianceband_{}'.format(i) for i in range(23)],\n",
    "    'kurtosis_rythm':['kurtosisband_{}'.format(i) for i in range(23)],\n",
    "    'skewness_rythm':['skewnessband_{}'.format(i) for i in range(23)],\n",
    "    'min_rythm':['minband_{}'.format(i) for i in range(23)],\n",
    "    'max_rythm':['maxband_{}'.format(i) for i in range(23)],\n",
    "    'mean_chroma':['meanpitch_{}'.format(i) for i in range(12)],\n",
    "    'std_chroma':['stdpitch_{}'.format(i) for i in range(12)],\n",
    "    'min_chroma':['minpitch_{}'.format(i) for i in range(12)],\n",
    "    'max_chroma':['maxpitch_{}'.format(i) for i in range(12)],\n",
    "    'mean_mfcc':['meanMFCC_{}'.format(i) for i in range(4,12)],\n",
    "    'std_mfcc':['stdMFCC_{}'.format(i) for i in range(12)],\n",
    "    'min_mfcc':['minMFCC_{}'.format(i) for i in range(12)],\n",
    "    'max_mfcc':['maxMFCC_{}'.format(i) for i in range(12)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prince\n",
    "mfa1 = prince.MFA(\n",
    "        groups=groups,\n",
    "        n_components=13\n",
    "        n_iter=3,\n",
    "        copy=True,\n",
    "        engine='auto',\n",
    "        random_state=42\n",
    ")\n",
    "mfa1.fit(train_processed)\n",
    "\n",
    "mfa_train = mfa.transform(train_processed)\n",
    "mfa_test = mfa.transform(test_processed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
